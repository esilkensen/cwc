\documentclass[11pt]{article}

\usepackage{fancyhdr}
\usepackage[margin=1.5in]{geometry}
\usepackage[numbers]{natbib}
\usepackage{stmaryrd}

\renewcommand{\bibsection}{\begin{flushleft}\textbf{\refname}\end{flushleft}}

\lhead{The Essence of Compiling with Continuations}
\rhead{Phillip Mates and Erik Silkensen}

\begin{document}

\thispagestyle{fancy}

Compilers for higher-order languages often transform programs into
continuation-passing style (CPS) form as an intermediate representation. In the
language of CPS terms, all procedures take a continuation argument $k$
that represents the ``rest of the computation;'' procedures apply $k$ to their
return value instead of returning directly. This form is beneficial to
compilers because it enables optimizations by way of simple $\beta$- and
$\eta$-reductions, and it simplifies code generation by resembling a low-level
target language.

While implementing a na\"{\i}ve CPS transformation is a straightforward task,
for realistic compilers the process typically involves at least two additional
phases. First, the CPS transformation introduces a large number of
``administrative'' terms which greatly increase a program's size.
A simplification phase reduces the resulting programs to a smaller, normal
form. Second, compilers treat the continuation closures specially, e.g., to
improve analyses or provide better allocation strategies.

\citet{Flanagan:1993fk} argue that by using a simpler organization based on
the $A$-reductions of \citet{Sabry:1992zr}, compilers could achieve the 
benefits of a fully developed CPS compiler with a single source-level
transformation. In support of their argument they present a sequence of
abstract machines simulating the compilation of a Scheme-like language.
They begin by deriving the CEK machine from the CE machine specialized for
(simplified) CPS, showing how to use the continuation register to model the
optimizations of realistic CPS compilers. Next they introduce the
$A$-reductions and the corresponding CEK machine, which they prove is
equivalent to the CEK machine for CPS. In fact, the machines are identical
up to the syntax of their control strings, and applying an inverse CPS
transformation to a simplified CPS term gives the $A$-normal form of the
original source-language term. They conclude that compiling with $A$-normal
forms realizes the essence of compiling with continuations, without requiring
a multi-stage CPS transformation.

While we understood \citeauthor{Flanagan:1993fk}'s reasoning at a high level
after an initial reading, we planned to try and grasp the subtleties of the
abstract machines and language transformations by implementing them in Redex.
Specifically, we were interested in better understanding how programs change
throughout the transformations and how redundancies are removed from the
CEK machine for CPS. We also wanted to implement the $A$-normalization
algorithm as a reduction relation using the evaluation contexts defined in the
paper, whose presentation appeared puzzling to us at first.

We transcribed the figures in the paper into language definitions,
metafunctions, and reduction relations in Redex. By running the transformations
on concrete examples, we gained more of an intuition for how they work. After
reading over the CEK machines a second time in order to implement them, we were
able to see the redundancies the authors discuss---how the machine for CPS
ignores the explicit continuation closures and instead manipulates the
continuation register. For the $A$-normalization algorithm, we included
additional rules to allow reducible expressions to appear in the bodies of
\textbf{let} and \textbf{if0} terms, for example, since these cases weren't
explicitly covered by the evaluation contexts defined in the paper.
Alternatively, we could have defined a metafunction according to the code in
the paper's appendix.

As a side effect, this project also helped us discover how a tool such as Redex
is useful to programming language researchers in the real world. For example,
as we began implementing a model of the paper, we noticed two small typos. In
particular, for the na\"{i}ve transformation (where $\mathcal{F}$ is due
to~\citet{Fischer:1993ys}), $\mathcal{F}\llbracket{M\rrbracket}$ should be
$(\mathcal{F}\llbracket{M\rrbracket}\ k)$ in
$\Phi\llbracket{\lambda{x_1 \ldots x_n}.M\rrbracket} =
\lambda{k x_1 \ldots x_n}.\mathcal{F}\llbracket{M\rrbracket};$
and for the inverse transformation,
$\mathcal{U}\llbracket{M\rrbracket}$ should be
$\mathcal{U}\llbracket{P\rrbracket}$ in
$\Psi\llbracket{\lambda{k x_1 \ldots x_n}.P\rrbracket} =
\lambda{x_1 \ldots x_n}.\mathcal{U}\llbracket{M\rrbracket}.$
These typos are easy to catch when one tries running the transformations as
they appear in the paper; Redex can render typeset figures of models, which
authors could use to avoid introducing errors (perhaps such as these) during
manual transcription into \LaTeX.
%
% \citet{Klein:2012uq} 

% New Proposal format:
%   (1) the fixed-up summary from the "proposal" memo
%   (2) a revised and concrete set of questions that you tackled
%   (3) a description of the approach used to answer them
%   (4) your answers (abstractly, because you have a limited amount of space)

% Questions asked:
%  - how programs change throughout the transformations?
%  - how redundancies are removed from the machines?
%  - CPS vs A in a compiler (timing)?
%  - how to implement A-reductions algorithm?
%  - are there typos in the figures?

\bibliographystyle{abbrvnat}
\bibliography{refs}

\end{document}
